New features:
- PopTorch now exposes PopART anchor options to choose how much data to return from a model. These
are passed into the model wrapper via |anchor_mode| options are Sum, All, Final and EveryN.
    All: Return a result for each batch.
    Sum: Return the sum of all the batches
    Final: Return the last batch.
    EveryN: Return every N batches. N is passed in as |anchor_return_period|

- Add support for batched LSTM and batch first

- An Options object can now be passed to poptorch.trainingModel / poptorch.inferenceModel to configure the session and select IPUs

- Adds support for the torch comparisons operations:
    torch.eq, torch.ge, torch.gt,
    torch.le, torch.lt, torch.max,
    torch.min, torch.ne, torch.isnan

    torch.min and torch.max only support (tensor, tensor) and (tensor) overloads. They do
    not support the (tensor, dim=, keepdim=) overload.

- Automatically synchronise the weights back to the Host after using the IPU for training. (i.e no need to explicitly call copyWeightsToHost() anymore)

- Adds support for torch.nn.PReLU

- Adds support for Adam optimizer.

- Adds support for half type models and inputs.

- Adds support for user provided custom operations. See PopART documentation for information on
  how to write them. They are exposed by `poptorch.custom_op` this takes in a list of
  input tensors, strings for the PopART op name and domain, the domain version, and
  a list of tensors the same shape and size as the expected output tensors. This is to
  ensure the pytorch trace remains valid as it traces on CPU so won't actually execute
  the operation when building the graph.